{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 關於Keras序列到序列學習的十分鐘介紹\n",
    "\n",
    "我經常看到有人問這個問題 - 如何在Keras中實現RNN序列到序列(sequence-to-sequence)的學習？\n",
    "\n",
    "這篇文章是對\"sequence-to-sequence\"一個簡短的介紹。\n",
    "\n",
    "請注意，這篇文章假設你已經有一些遞歸網絡(recurrent networks)和Keras的經驗。\n",
    "\n",
    "![seq2seq](http://pytorch.org/tutorials/_images/seq2seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什麼是從序列到序列 (seq2seq) 的學習？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列到序列（Seq2Seq）學習是關於訓練模型以將來自一個領域（例如，英語的句子）的序列轉換成另一個領域（例如翻譯成中文的相同句子）的序列的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\"the cat sat on the mat\" -> [Seq2Seq model] -> \"那隻貓坐在地毯上\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這可以用於機器翻譯或任何Q&A（根據自然語言問題生成自然語言答案） - 通常，只要您需要生成文本，就可以使用它。\n",
    "\n",
    "有多種方式來處理這樣的任務，或者使用RNN或者使用一維的卷積網絡(convnets)。這裡我們將重點放在RNN的使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 簡單的案例：當輸入和輸出序列具有相同的長度時\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "當輸入序列和輸出序列長度相同時，您可以簡單地用Keras LSTM或GRU層（或其堆疊）來實現這些模型。以下的示範就是這種情況，它顯示瞭如何教導RNN學習如何對數字進行相加(加法)：\n",
    "\n",
    "![addition](https://blog.keras.io/img/seq2seq/addition-rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1. 引入相關的函數庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    \"\"\"\n",
    "    給予一組的字符:\n",
    "    + 將這些字符使用one-hot編碼成數字表示\n",
    "    + 解碼one-hot編碼數字表示成為原本的字符\n",
    "    + 解碼字符機率的向量以回覆最有可能的字符\n",
    "    \"\"\"\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"初始化字符表\n",
    "        \n",
    "        # 參數:\n",
    "            chars: 會出現在輸入的可能字符集\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        \n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"對輸入的字串進行one-hot編碼\n",
    "        \n",
    "        # 參數:\n",
    "            C: 要被編碼的字符\n",
    "            num_rows: one-hot編碼後要回傳的最大行數。這是用來確保每一個輸入都會得到\n",
    "            相同行數的輸出\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1 #这里赋值为1是把里面的False变为True\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        \"\"\"對輸入的編碼(向量)進行解碼\n",
    "        \n",
    "        # 參數:\n",
    "            x: 要被解碼的字符向量或字符編碼\n",
    "            calc_argmax: 是否要用argmax算符找出機率最大的字符編碼\n",
    "        \"\"\"\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x)\n",
    "    \n",
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2. 相關的參數與產生訓練用的資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型與資料集的參數\n",
    "TRAINING_SIZE = 50000 # 訓練資料集的samples數\n",
    "DIGITS = 3            # 加數或被加數的字符數\n",
    "INVERT = True \n",
    "\n",
    "# 輸入的最大長度 'int + int' (比如, '345+678')\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "\n",
    "# 所有要用到的字符(包括數字、加號及空格)\n",
    "chars = '0123456789+ '\n",
    "ctable = CharacterTable(chars) # 創建CharacterTable的instance\n",
    "\n",
    "questions = [] # 訓練用的句子 \"xxx+yyy\"\n",
    "expected = []  # 訓練用的標籤\n",
    "seen = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in range(np.random.randint(1, DIGITS+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5', '7', '3']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.random.choice(list('0123456789')) for i in range(np.random.randint(1, DIGITS+1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 50000\n"
     ]
    }
   ],
   "source": [
    "print('Generating data...') # 產生訓練資料\n",
    "\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    # 數字產生器 (3個字符)\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in range(np.random.randint(1, DIGITS+1))))\n",
    "    \n",
    "    a, b = f(), f()\n",
    "    # 跳過己經看過的題目以及x+Y = Y+x這樣的題目\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue    \n",
    "    seen.add(key)\n",
    "    \n",
    "    # 當數字不足MAXLEN則填補空白\n",
    "    q = '{}+{}'.format(a, b)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    \n",
    "    # 答案的最大的字符長度為DIGITS + 1\n",
    "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "    if INVERT:\n",
    "        # 調轉問題字符的方向, 比如. '12+345'變成'543+21'\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "    \n",
    "print('Total addition questions:', len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 12)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions),len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# 把資料做適當的轉換, LSTM預期的資料結構 -> [samples, timesteps, features]\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool) # 初始一個3維的numpy ndarray (特徵資料)\n",
    "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool) # 初始一個3維的numpy ndarray (標籤資料)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 7, 12)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "Feature data:  (50000, 7, 12)\n",
      "Label data:  (50000, 4, 12)\n"
     ]
    }
   ],
   "source": [
    "# 把資料做適當的轉換, LSTM預期的資料結構 -> [samples, timesteps, features]\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool) # 初始一個3維的numpy ndarray (特徵資料)\n",
    "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool) # 初始一個3維的numpy ndarray (標籤資料)\n",
    "\n",
    "# 將\"特徵資料\"轉換成LSTM預期的資料結構 -> [samples, timesteps, features]\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)      # <--- 要了解為什麼要這樣整理資料\n",
    "\n",
    "print(\"Feature data: \", x.shape)\n",
    "\n",
    "# 將\"標籤資料\"轉換成LSTM預期的資料結構 -> [samples, timesteps, features]\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, DIGITS + 1)  # <--- 要了解為什麼要這樣整理資料\n",
    "\n",
    "print(\"Label data: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "(45000, 7, 12)\n",
      "(45000, 4, 12)\n",
      "Validation Data:\n",
      "(5000, 7, 12)\n",
      "(5000, 4, 12)\n"
     ]
    }
   ],
   "source": [
    "# 打散 Shuffle(x, y)\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# 保留10%的資料來做為驗證\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Validation Data:')\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4.構建網絡架構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以試著替代其它種的rnn units, 比如,GRU或SimpleRNN\n",
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 編碼 (encoder) ====\n",
    "\n",
    "# 使用RNN“編碼”輸入序列，產生HIDDEN_SIZE的輸出。\n",
    "# 注意：在輸入序列長度可變的情況下，使用input_shape =（None，num_features）\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars)))) # MAXLEN代表是timesteps, 而len(chars)是one-hot編碼的features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对于RepeatVector的解释并不清楚，简单的说就是定义输出样本长度为4。建议查看[中文文档](https://keras.io/zh/layers/core/#repeatvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作為解碼器RNN的輸入，重複提供每個時間步的RNN的最後一個隱藏狀態。\n",
    "# 重複“DIGITS + 1”次，因為這是最大輸出長度，例如當DIGITS = 3時，最大輸出是999 + 999 = 1998（長度為4)。\n",
    "model.add(layers.RepeatVector(DIGITS+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 解碼 (decoder) ====\n",
    "# 解碼器RNN可以是多層堆疊或單層。\n",
    "for _ in range(LAYERS):\n",
    "    # 通過將return_sequences設置為True，不僅返回最後一個輸出，而且還以（num_samples，timesteps，output_dim）\n",
    "    # 的形式返回所有輸出。這是必要的，因為下面的TimeDistributed需要第一個維度是時間步長。\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于timedistributed的解释还是看[中文文档](https://keras.io/zh/layers/wrappers/#timedistributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 128)               72192     \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 4, 128)            131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 4, 12)             1548      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4, 12)             0         \n",
      "=================================================================\n",
      "Total params: 205,324\n",
      "Trainable params: 205,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 對輸入的每個時間片推送到密集層來對於輸出序列的每一時間步，決定選擇哪個字符。\n",
    "model.add(layers.TimeDistributed(layers.Dense(len(chars))))\n",
    "\n",
    "model.add(layers.Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5.訓練模型/驗證評估\n",
    "\n",
    " 我們將進行50次的訓練，並且在每次訓練之後就進行檢查。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 18s 392us/step - loss: 1.8803 - acc: 0.3225 - val_loss: 1.7717 - val_acc: 0.3458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23894a7de80>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,batch_size=BATCH_SIZE,epochs=1,validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.random.randint(0, len(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ True, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [ True, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False,  True,\n",
       "         False, False, False],\n",
       "        [False,  True, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,\n",
       "          True, False, False],\n",
       "        [False, False, False, False, False, False, False, False,  True,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False,  True, False, False, False,\n",
       "         False, False, False]]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rowx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 7, 12)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 15s 324us/step - loss: 1.7271 - acc: 0.3617 - val_loss: 1.6537 - val_acc: 0.3885\n",
      "Q 72+63   T 135  \u001b[91m☒\u001b[0m 777 \n",
      "Q 74+594  T 668  \u001b[91m☒\u001b[0m 904 \n",
      "Q 2+81    T 83   \u001b[91m☒\u001b[0m 11  \n",
      "Q 819+902 T 1721 \u001b[91m☒\u001b[0m 1414\n",
      "Q 553+87  T 640  \u001b[91m☒\u001b[0m 604 \n",
      "Q 899+48  T 947  \u001b[91m☒\u001b[0m 904 \n",
      "Q 2+477   T 479  \u001b[91m☒\u001b[0m 334 \n",
      "Q 14+16   T 30   \u001b[91m☒\u001b[0m 11  \n",
      "Q 84+152  T 236  \u001b[91m☒\u001b[0m 606 \n",
      "Q 40+261  T 301  \u001b[91m☒\u001b[0m 366 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 15s 341us/step - loss: 1.5749 - acc: 0.4106 - val_loss: 1.4898 - val_acc: 0.4420\n",
      "Q 49+330  T 379  \u001b[91m☒\u001b[0m 477 \n",
      "Q 70+77   T 147  \u001b[91m☒\u001b[0m 174 \n",
      "Q 32+764  T 796  \u001b[91m☒\u001b[0m 787 \n",
      "Q 524+368 T 892  \u001b[91m☒\u001b[0m 108 \n",
      "Q 146+93  T 239  \u001b[91m☒\u001b[0m 574 \n",
      "Q 677+0   T 677  \u001b[91m☒\u001b[0m 777 \n",
      "Q 152+431 T 583  \u001b[91m☒\u001b[0m 887 \n",
      "Q 354+0   T 354  \u001b[91m☒\u001b[0m 444 \n",
      "Q 5+874   T 879  \u001b[91m☒\u001b[0m 874 \n",
      "Q 94+198  T 292  \u001b[91m☒\u001b[0m 122 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 1.4026 - acc: 0.4744 - val_loss: 1.3166 - val_acc: 0.5066\n",
      "Q 58+58   T 116  \u001b[91m☒\u001b[0m 122 \n",
      "Q 532+354 T 886  \u001b[91m☒\u001b[0m 806 \n",
      "Q 525+197 T 722  \u001b[91m☒\u001b[0m 666 \n",
      "Q 796+33  T 829  \u001b[91m☒\u001b[0m 804 \n",
      "Q 4+28    T 32   \u001b[91m☒\u001b[0m 33  \n",
      "Q 598+96  T 694  \u001b[91m☒\u001b[0m 633 \n",
      "Q 56+71   T 127  \u001b[91m☒\u001b[0m 121 \n",
      "Q 523+604 T 1127 \u001b[91m☒\u001b[0m 1106\n",
      "Q 13+478  T 491  \u001b[91m☒\u001b[0m 412 \n",
      "Q 583+52  T 635  \u001b[91m☒\u001b[0m 693 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 1.2606 - acc: 0.5301 - val_loss: 1.2012 - val_acc: 0.5510\n",
      "Q 11+997  T 1008 \u001b[91m☒\u001b[0m 1011\n",
      "Q 416+63  T 479  \u001b[91m☒\u001b[0m 474 \n",
      "Q 11+48   T 59   \u001b[91m☒\u001b[0m 77  \n",
      "Q 708+991 T 1699 \u001b[91m☒\u001b[0m 1624\n",
      "Q 835+26  T 861  \u001b[91m☒\u001b[0m 865 \n",
      "Q 983+63  T 1046 \u001b[91m☒\u001b[0m 1034\n",
      "Q 8+898   T 906  \u001b[91m☒\u001b[0m 999 \n",
      "Q 700+97  T 797  \u001b[92m☑\u001b[0m 797 \n",
      "Q 9+410   T 419  \u001b[91m☒\u001b[0m 414 \n",
      "Q 0+6     T 6    \u001b[91m☒\u001b[0m 7   \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 1.1520 - acc: 0.5751 - val_loss: 1.0930 - val_acc: 0.5958\n",
      "Q 188+8   T 196  \u001b[91m☒\u001b[0m 292 \n",
      "Q 76+620  T 696  \u001b[91m☒\u001b[0m 693 \n",
      "Q 212+294 T 506  \u001b[91m☒\u001b[0m 410 \n",
      "Q 521+82  T 603  \u001b[91m☒\u001b[0m 693 \n",
      "Q 199+815 T 1014 \u001b[91m☒\u001b[0m 100 \n",
      "Q 329+0   T 329  \u001b[91m☒\u001b[0m 332 \n",
      "Q 750+56  T 806  \u001b[91m☒\u001b[0m 812 \n",
      "Q 712+271 T 983  \u001b[91m☒\u001b[0m 905 \n",
      "Q 40+486  T 526  \u001b[91m☒\u001b[0m 534 \n",
      "Q 8+684   T 692  \u001b[91m☒\u001b[0m 686 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 15s 327us/step - loss: 1.0495 - acc: 0.6166 - val_loss: 1.0001 - val_acc: 0.6433\n",
      "Q 84+242  T 326  \u001b[91m☒\u001b[0m 320 \n",
      "Q 899+976 T 1875 \u001b[91m☒\u001b[0m 1807\n",
      "Q 213+332 T 545  \u001b[91m☒\u001b[0m 555 \n",
      "Q 99+882  T 981  \u001b[91m☒\u001b[0m 977 \n",
      "Q 68+426  T 494  \u001b[91m☒\u001b[0m 496 \n",
      "Q 725+562 T 1287 \u001b[91m☒\u001b[0m 1201\n",
      "Q 897+98  T 995  \u001b[91m☒\u001b[0m 977 \n",
      "Q 12+574  T 586  \u001b[91m☒\u001b[0m 588 \n",
      "Q 290+660 T 950  \u001b[91m☒\u001b[0m 900 \n",
      "Q 3+540   T 543  \u001b[92m☑\u001b[0m 543 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 15s 329us/step - loss: 0.9617 - acc: 0.6550 - val_loss: 0.9351 - val_acc: 0.6586\n",
      "Q 814+479 T 1293 \u001b[91m☒\u001b[0m 1299\n",
      "Q 86+600  T 686  \u001b[91m☒\u001b[0m 688 \n",
      "Q 44+341  T 385  \u001b[91m☒\u001b[0m 388 \n",
      "Q 307+610 T 917  \u001b[91m☒\u001b[0m 922 \n",
      "Q 108+71  T 179  \u001b[91m☒\u001b[0m 170 \n",
      "Q 151+53  T 204  \u001b[91m☒\u001b[0m 200 \n",
      "Q 70+46   T 116  \u001b[91m☒\u001b[0m 110 \n",
      "Q 796+33  T 829  \u001b[91m☒\u001b[0m 822 \n",
      "Q 73+257  T 330  \u001b[91m☒\u001b[0m 320 \n",
      "Q 929+44  T 973  \u001b[91m☒\u001b[0m 978 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 320us/step - loss: 0.8861 - acc: 0.6840 - val_loss: 0.8454 - val_acc: 0.6994\n",
      "Q 414+40  T 454  \u001b[91m☒\u001b[0m 458 \n",
      "Q 609+52  T 661  \u001b[91m☒\u001b[0m 663 \n",
      "Q 48+868  T 916  \u001b[91m☒\u001b[0m 911 \n",
      "Q 877+933 T 1810 \u001b[91m☒\u001b[0m 1712\n",
      "Q 932+7   T 939  \u001b[91m☒\u001b[0m 931 \n",
      "Q 474+916 T 1390 \u001b[91m☒\u001b[0m 1388\n",
      "Q 169+36  T 205  \u001b[91m☒\u001b[0m 204 \n",
      "Q 75+21   T 96   \u001b[91m☒\u001b[0m 90  \n",
      "Q 1+739   T 740  \u001b[91m☒\u001b[0m 730 \n",
      "Q 63+336  T 399  \u001b[91m☒\u001b[0m 400 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 15s 327us/step - loss: 0.8107 - acc: 0.7148 - val_loss: 0.7744 - val_acc: 0.7350\n",
      "Q 5+874   T 879  \u001b[91m☒\u001b[0m 878 \n",
      "Q 459+92  T 551  \u001b[91m☒\u001b[0m 541 \n",
      "Q 592+11  T 603  \u001b[92m☑\u001b[0m 603 \n",
      "Q 371+77  T 448  \u001b[92m☑\u001b[0m 448 \n",
      "Q 75+953  T 1028 \u001b[91m☒\u001b[0m 1036\n",
      "Q 17+956  T 973  \u001b[91m☒\u001b[0m 978 \n",
      "Q 234+16  T 250  \u001b[91m☒\u001b[0m 258 \n",
      "Q 25+41   T 66   \u001b[91m☒\u001b[0m 65  \n",
      "Q 654+9   T 663  \u001b[92m☑\u001b[0m 663 \n",
      "Q 0+467   T 467  \u001b[91m☒\u001b[0m 468 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 10\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 319us/step - loss: 0.7467 - acc: 0.7393 - val_loss: 0.7144 - val_acc: 0.7511\n",
      "Q 375+184 T 559  \u001b[91m☒\u001b[0m 555 \n",
      "Q 15+822  T 837  \u001b[91m☒\u001b[0m 843 \n",
      "Q 29+160  T 189  \u001b[91m☒\u001b[0m 188 \n",
      "Q 164+6   T 170  \u001b[92m☑\u001b[0m 170 \n",
      "Q 92+516  T 608  \u001b[91m☒\u001b[0m 605 \n",
      "Q 1+538   T 539  \u001b[91m☒\u001b[0m 530 \n",
      "Q 325+28  T 353  \u001b[91m☒\u001b[0m 352 \n",
      "Q 814+995 T 1809 \u001b[91m☒\u001b[0m 1813\n",
      "Q 24+72   T 96   \u001b[91m☒\u001b[0m 90  \n",
      "Q 903+2   T 905  \u001b[92m☑\u001b[0m 905 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 11\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 313us/step - loss: 0.6656 - acc: 0.7713 - val_loss: 0.6109 - val_acc: 0.7907\n",
      "Q 89+266  T 355  \u001b[91m☒\u001b[0m 354 \n",
      "Q 292+713 T 1005 \u001b[91m☒\u001b[0m 100 \n",
      "Q 924+346 T 1270 \u001b[91m☒\u001b[0m 1269\n",
      "Q 313+50  T 363  \u001b[92m☑\u001b[0m 363 \n",
      "Q 408+10  T 418  \u001b[92m☑\u001b[0m 418 \n",
      "Q 165+819 T 984  \u001b[91m☒\u001b[0m 985 \n",
      "Q 152+85  T 237  \u001b[91m☒\u001b[0m 238 \n",
      "Q 994+294 T 1288 \u001b[92m☑\u001b[0m 1288\n",
      "Q 207+33  T 240  \u001b[92m☑\u001b[0m 240 \n",
      "Q 139+6   T 145  \u001b[92m☑\u001b[0m 145 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 12\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 306us/step - loss: 0.5026 - acc: 0.8348 - val_loss: 0.4319 - val_acc: 0.8587\n",
      "Q 6+57    T 63   \u001b[91m☒\u001b[0m 62  \n",
      "Q 302+194 T 496  \u001b[92m☑\u001b[0m 496 \n",
      "Q 614+541 T 1155 \u001b[91m☒\u001b[0m 1166\n",
      "Q 35+35   T 70   \u001b[91m☒\u001b[0m 61  \n",
      "Q 489+19  T 508  \u001b[91m☒\u001b[0m 596 \n",
      "Q 30+608  T 638  \u001b[92m☑\u001b[0m 638 \n",
      "Q 591+95  T 686  \u001b[91m☒\u001b[0m 676 \n",
      "Q 414+194 T 608  \u001b[92m☑\u001b[0m 608 \n",
      "Q 39+283  T 322  \u001b[92m☑\u001b[0m 322 \n",
      "Q 41+20   T 61   \u001b[91m☒\u001b[0m 62  \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 13\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 302us/step - loss: 0.3625 - acc: 0.8958 - val_loss: 0.3114 - val_acc: 0.9189\n",
      "Q 191+862 T 1053 \u001b[92m☑\u001b[0m 1053\n",
      "Q 75+385  T 460  \u001b[92m☑\u001b[0m 460 \n",
      "Q 368+8   T 376  \u001b[91m☒\u001b[0m 375 \n",
      "Q 151+15  T 166  \u001b[92m☑\u001b[0m 166 \n",
      "Q 584+35  T 619  \u001b[92m☑\u001b[0m 619 \n",
      "Q 929+84  T 1013 \u001b[92m☑\u001b[0m 1013\n",
      "Q 258+961 T 1219 \u001b[92m☑\u001b[0m 1219\n",
      "Q 8+219   T 227  \u001b[92m☑\u001b[0m 227 \n",
      "Q 99+658  T 757  \u001b[91m☒\u001b[0m 755 \n",
      "Q 122+65  T 187  \u001b[92m☑\u001b[0m 187 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 14\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 303us/step - loss: 0.2653 - acc: 0.9360 - val_loss: 0.2483 - val_acc: 0.9329\n",
      "Q 388+11  T 399  \u001b[92m☑\u001b[0m 399 \n",
      "Q 78+412  T 490  \u001b[91m☒\u001b[0m 480 \n",
      "Q 740+539 T 1279 \u001b[92m☑\u001b[0m 1279\n",
      "Q 5+851   T 856  \u001b[91m☒\u001b[0m 855 \n",
      "Q 449+245 T 694  \u001b[92m☑\u001b[0m 694 \n",
      "Q 67+239  T 306  \u001b[92m☑\u001b[0m 306 \n",
      "Q 125+712 T 837  \u001b[92m☑\u001b[0m 837 \n",
      "Q 198+748 T 946  \u001b[91m☒\u001b[0m 944 \n",
      "Q 46+127  T 173  \u001b[92m☑\u001b[0m 173 \n",
      "Q 239+41  T 280  \u001b[92m☑\u001b[0m 280 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 15\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 14s 311us/step - loss: 0.2007 - acc: 0.9577 - val_loss: 0.1837 - val_acc: 0.9591\n",
      "Q 66+919  T 985  \u001b[92m☑\u001b[0m 985 \n",
      "Q 434+17  T 451  \u001b[92m☑\u001b[0m 451 \n",
      "Q 78+412  T 490  \u001b[92m☑\u001b[0m 490 \n",
      "Q 383+18  T 401  \u001b[92m☑\u001b[0m 401 \n",
      "Q 659+95  T 754  \u001b[92m☑\u001b[0m 754 \n",
      "Q 61+639  T 700  \u001b[92m☑\u001b[0m 700 \n",
      "Q 255+96  T 351  \u001b[92m☑\u001b[0m 351 \n",
      "Q 86+464  T 550  \u001b[92m☑\u001b[0m 550 \n",
      "Q 762+13  T 775  \u001b[92m☑\u001b[0m 775 \n",
      "Q 55+43   T 98   \u001b[91m☒\u001b[0m 99  \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 16\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 309us/step - loss: 0.1553 - acc: 0.9701 - val_loss: 0.1451 - val_acc: 0.9721\n",
      "Q 174+459 T 633  \u001b[92m☑\u001b[0m 633 \n",
      "Q 592+239 T 831  \u001b[92m☑\u001b[0m 831 \n",
      "Q 985+51  T 1036 \u001b[92m☑\u001b[0m 1036\n",
      "Q 532+196 T 728  \u001b[92m☑\u001b[0m 728 \n",
      "Q 64+53   T 117  \u001b[92m☑\u001b[0m 117 \n",
      "Q 48+251  T 299  \u001b[92m☑\u001b[0m 299 \n",
      "Q 97+222  T 319  \u001b[92m☑\u001b[0m 319 \n",
      "Q 248+710 T 958  \u001b[92m☑\u001b[0m 958 \n",
      "Q 571+2   T 573  \u001b[92m☑\u001b[0m 573 \n",
      "Q 470+72  T 542  \u001b[92m☑\u001b[0m 542 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 17\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 313us/step - loss: 0.1233 - acc: 0.9775 - val_loss: 0.1086 - val_acc: 0.9811\n",
      "Q 816+20  T 836  \u001b[92m☑\u001b[0m 836 \n",
      "Q 802+678 T 1480 \u001b[92m☑\u001b[0m 1480\n",
      "Q 0+127   T 127  \u001b[92m☑\u001b[0m 127 \n",
      "Q 747+80  T 827  \u001b[92m☑\u001b[0m 827 \n",
      "Q 940+25  T 965  \u001b[92m☑\u001b[0m 965 \n",
      "Q 80+734  T 814  \u001b[92m☑\u001b[0m 814 \n",
      "Q 591+95  T 686  \u001b[92m☑\u001b[0m 686 \n",
      "Q 236+411 T 647  \u001b[92m☑\u001b[0m 647 \n",
      "Q 566+376 T 942  \u001b[91m☒\u001b[0m 943 \n",
      "Q 224+78  T 302  \u001b[92m☑\u001b[0m 302 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 18\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 316us/step - loss: 0.0896 - acc: 0.9873 - val_loss: 0.1107 - val_acc: 0.9743\n",
      "Q 63+384  T 447  \u001b[92m☑\u001b[0m 447 \n",
      "Q 38+53   T 91   \u001b[92m☑\u001b[0m 91  \n",
      "Q 271+776 T 1047 \u001b[91m☒\u001b[0m 1147\n",
      "Q 70+676  T 746  \u001b[92m☑\u001b[0m 746 \n",
      "Q 457+15  T 472  \u001b[92m☑\u001b[0m 472 \n",
      "Q 79+442  T 521  \u001b[92m☑\u001b[0m 521 \n",
      "Q 198+255 T 453  \u001b[92m☑\u001b[0m 453 \n",
      "Q 84+9    T 93   \u001b[92m☑\u001b[0m 93  \n",
      "Q 42+240  T 282  \u001b[92m☑\u001b[0m 282 \n",
      "Q 215+0   T 215  \u001b[92m☑\u001b[0m 215 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 19\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 15s 323us/step - loss: 0.0823 - acc: 0.9855 - val_loss: 0.0999 - val_acc: 0.9762\n",
      "Q 45+807  T 852  \u001b[92m☑\u001b[0m 852 \n",
      "Q 6+461   T 467  \u001b[92m☑\u001b[0m 467 \n",
      "Q 568+75  T 643  \u001b[92m☑\u001b[0m 643 \n",
      "Q 1+933   T 934  \u001b[92m☑\u001b[0m 934 \n",
      "Q 860+65  T 925  \u001b[92m☑\u001b[0m 925 \n",
      "Q 92+131  T 223  \u001b[92m☑\u001b[0m 223 \n",
      "Q 52+934  T 986  \u001b[92m☑\u001b[0m 986 \n",
      "Q 928+3   T 931  \u001b[92m☑\u001b[0m 931 \n",
      "Q 68+65   T 133  \u001b[92m☑\u001b[0m 133 \n",
      "Q 781+64  T 845  \u001b[92m☑\u001b[0m 845 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 20\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 313us/step - loss: 0.0627 - acc: 0.9903 - val_loss: 0.0681 - val_acc: 0.9856\n",
      "Q 51+518  T 569  \u001b[92m☑\u001b[0m 569 \n",
      "Q 91+935  T 1026 \u001b[92m☑\u001b[0m 1026\n",
      "Q 515+711 T 1226 \u001b[92m☑\u001b[0m 1226\n",
      "Q 426+564 T 990  \u001b[92m☑\u001b[0m 990 \n",
      "Q 881+1   T 882  \u001b[91m☒\u001b[0m 883 \n",
      "Q 34+6    T 40   \u001b[91m☒\u001b[0m 30  \n",
      "Q 79+664  T 743  \u001b[92m☑\u001b[0m 743 \n",
      "Q 881+1   T 882  \u001b[91m☒\u001b[0m 883 \n",
      "Q 21+648  T 669  \u001b[92m☑\u001b[0m 669 \n",
      "Q 614+541 T 1155 \u001b[92m☑\u001b[0m 1155\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 21\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 313us/step - loss: 0.0592 - acc: 0.9891 - val_loss: 0.0956 - val_acc: 0.9715\n",
      "Q 28+78   T 106  \u001b[92m☑\u001b[0m 106 \n",
      "Q 81+595  T 676  \u001b[92m☑\u001b[0m 676 \n",
      "Q 151+15  T 166  \u001b[92m☑\u001b[0m 166 \n",
      "Q 70+257  T 327  \u001b[92m☑\u001b[0m 327 \n",
      "Q 0+840   T 840  \u001b[92m☑\u001b[0m 840 \n",
      "Q 44+45   T 89   \u001b[92m☑\u001b[0m 89  \n",
      "Q 558+67  T 625  \u001b[92m☑\u001b[0m 625 \n",
      "Q 558+67  T 625  \u001b[92m☑\u001b[0m 625 \n",
      "Q 383+757 T 1140 \u001b[92m☑\u001b[0m 1140\n",
      "Q 557+34  T 591  \u001b[92m☑\u001b[0m 591 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 22\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 311us/step - loss: 0.0465 - acc: 0.9925 - val_loss: 0.0396 - val_acc: 0.9943\n",
      "Q 4+377   T 381  \u001b[92m☑\u001b[0m 381 \n",
      "Q 978+234 T 1212 \u001b[92m☑\u001b[0m 1212\n",
      "Q 68+608  T 676  \u001b[92m☑\u001b[0m 676 \n",
      "Q 671+852 T 1523 \u001b[92m☑\u001b[0m 1523\n",
      "Q 374+77  T 451  \u001b[92m☑\u001b[0m 451 \n",
      "Q 41+335  T 376  \u001b[92m☑\u001b[0m 376 \n",
      "Q 164+6   T 170  \u001b[92m☑\u001b[0m 170 \n",
      "Q 357+262 T 619  \u001b[92m☑\u001b[0m 619 \n",
      "Q 530+0   T 530  \u001b[92m☑\u001b[0m 530 \n",
      "Q 799+68  T 867  \u001b[92m☑\u001b[0m 867 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 23\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 15s 327us/step - loss: 0.0310 - acc: 0.9967 - val_loss: 0.0369 - val_acc: 0.9931\n",
      "Q 553+78  T 631  \u001b[92m☑\u001b[0m 631 \n",
      "Q 614+541 T 1155 \u001b[92m☑\u001b[0m 1155\n",
      "Q 834+971 T 1805 \u001b[92m☑\u001b[0m 1805\n",
      "Q 246+292 T 538  \u001b[92m☑\u001b[0m 538 \n",
      "Q 559+1   T 560  \u001b[92m☑\u001b[0m 560 \n",
      "Q 347+0   T 347  \u001b[92m☑\u001b[0m 347 \n",
      "Q 2+724   T 726  \u001b[92m☑\u001b[0m 726 \n",
      "Q 41+399  T 440  \u001b[92m☑\u001b[0m 440 \n",
      "Q 8+23    T 31   \u001b[91m☒\u001b[0m 21  \n",
      "Q 3+825   T 828  \u001b[92m☑\u001b[0m 828 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 24\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 320us/step - loss: 0.0528 - acc: 0.9873 - val_loss: 0.0521 - val_acc: 0.9876\n",
      "Q 836+48  T 884  \u001b[92m☑\u001b[0m 884 \n",
      "Q 377+348 T 725  \u001b[92m☑\u001b[0m 725 \n",
      "Q 308+695 T 1003 \u001b[92m☑\u001b[0m 1003\n",
      "Q 83+589  T 672  \u001b[92m☑\u001b[0m 672 \n",
      "Q 772+790 T 1562 \u001b[92m☑\u001b[0m 1562\n",
      "Q 32+56   T 88   \u001b[92m☑\u001b[0m 88  \n",
      "Q 1+889   T 890  \u001b[91m☒\u001b[0m 880 \n",
      "Q 48+236  T 284  \u001b[92m☑\u001b[0m 284 \n",
      "Q 12+832  T 844  \u001b[92m☑\u001b[0m 844 \n",
      "Q 624+82  T 706  \u001b[92m☑\u001b[0m 706 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 25\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 309us/step - loss: 0.0244 - acc: 0.9973 - val_loss: 0.0259 - val_acc: 0.9961\n",
      "Q 2+432   T 434  \u001b[92m☑\u001b[0m 434 \n",
      "Q 813+44  T 857  \u001b[92m☑\u001b[0m 857 \n",
      "Q 7+337   T 344  \u001b[92m☑\u001b[0m 344 \n",
      "Q 28+550  T 578  \u001b[92m☑\u001b[0m 578 \n",
      "Q 880+445 T 1325 \u001b[92m☑\u001b[0m 1325\n",
      "Q 70+283  T 353  \u001b[92m☑\u001b[0m 353 \n",
      "Q 428+63  T 491  \u001b[92m☑\u001b[0m 491 \n",
      "Q 112+706 T 818  \u001b[92m☑\u001b[0m 818 \n",
      "Q 956+6   T 962  \u001b[92m☑\u001b[0m 962 \n",
      "Q 705+47  T 752  \u001b[92m☑\u001b[0m 752 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 26\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 317us/step - loss: 0.0297 - acc: 0.9939 - val_loss: 0.0644 - val_acc: 0.9784\n",
      "Q 33+152  T 185  \u001b[92m☑\u001b[0m 185 \n",
      "Q 826+74  T 900  \u001b[92m☑\u001b[0m 900 \n",
      "Q 410+243 T 653  \u001b[92m☑\u001b[0m 653 \n",
      "Q 32+1    T 33   \u001b[92m☑\u001b[0m 33  \n",
      "Q 42+680  T 722  \u001b[92m☑\u001b[0m 722 \n",
      "Q 15+812  T 827  \u001b[92m☑\u001b[0m 827 \n",
      "Q 792+891 T 1683 \u001b[91m☒\u001b[0m 1673\n",
      "Q 15+391  T 406  \u001b[92m☑\u001b[0m 406 \n",
      "Q 180+797 T 977  \u001b[91m☒\u001b[0m 9767\n",
      "Q 284+85  T 369  \u001b[92m☑\u001b[0m 369 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 27\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 15s 323us/step - loss: 0.0225 - acc: 0.9964 - val_loss: 0.0188 - val_acc: 0.9971\n",
      "Q 62+880  T 942  \u001b[92m☑\u001b[0m 942 \n",
      "Q 859+92  T 951  \u001b[92m☑\u001b[0m 951 \n",
      "Q 360+64  T 424  \u001b[92m☑\u001b[0m 424 \n",
      "Q 29+268  T 297  \u001b[91m☒\u001b[0m 397 \n",
      "Q 34+306  T 340  \u001b[92m☑\u001b[0m 340 \n",
      "Q 4+555   T 559  \u001b[92m☑\u001b[0m 559 \n",
      "Q 82+910  T 992  \u001b[92m☑\u001b[0m 992 \n",
      "Q 338+11  T 349  \u001b[92m☑\u001b[0m 349 \n",
      "Q 7+93    T 100  \u001b[92m☑\u001b[0m 100 \n",
      "Q 208+81  T 289  \u001b[92m☑\u001b[0m 289 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 28\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 305us/step - loss: 0.0293 - acc: 0.9932 - val_loss: 0.0223 - val_acc: 0.9962\n",
      "Q 436+799 T 1235 \u001b[92m☑\u001b[0m 1235\n",
      "Q 43+167  T 210  \u001b[92m☑\u001b[0m 210 \n",
      "Q 44+146  T 190  \u001b[92m☑\u001b[0m 190 \n",
      "Q 2+857   T 859  \u001b[92m☑\u001b[0m 859 \n",
      "Q 615+53  T 668  \u001b[92m☑\u001b[0m 668 \n",
      "Q 16+96   T 112  \u001b[92m☑\u001b[0m 112 \n",
      "Q 48+431  T 479  \u001b[92m☑\u001b[0m 479 \n",
      "Q 38+84   T 122  \u001b[92m☑\u001b[0m 122 \n",
      "Q 716+796 T 1512 \u001b[92m☑\u001b[0m 1512\n",
      "Q 92+102  T 194  \u001b[92m☑\u001b[0m 194 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 29\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 14s 306us/step - loss: 0.0149 - acc: 0.9981 - val_loss: 0.0152 - val_acc: 0.9979\n",
      "Q 394+51  T 445  \u001b[92m☑\u001b[0m 445 \n",
      "Q 257+7   T 264  \u001b[92m☑\u001b[0m 264 \n",
      "Q 564+38  T 602  \u001b[92m☑\u001b[0m 602 \n",
      "Q 73+162  T 235  \u001b[92m☑\u001b[0m 235 \n",
      "Q 532+96  T 628  \u001b[92m☑\u001b[0m 628 \n",
      "Q 277+286 T 563  \u001b[92m☑\u001b[0m 563 \n",
      "Q 65+417  T 482  \u001b[92m☑\u001b[0m 482 \n",
      "Q 209+67  T 276  \u001b[92m☑\u001b[0m 276 \n",
      "Q 2+130   T 132  \u001b[92m☑\u001b[0m 132 \n",
      "Q 133+569 T 702  \u001b[92m☑\u001b[0m 702 \n"
     ]
    }
   ],
   "source": [
    "for iteration in range(1, 30):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(x_train, y_train,\n",
    "             batch_size=BATCH_SIZE,\n",
    "             epochs=1,\n",
    "             validation_data=(x_val, y_val))\n",
    "    \n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = model.predict_classes(rowx, verbose=0)\n",
    "        \n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print('Q', q[::-1] if INVERT else q, end=' ')\n",
    "        print('T', correct, end=' ')\n",
    "        if correct == guess:\n",
    "            print(colors.ok + '☑' + colors.close, end=' ')\n",
    "        else:\n",
    "            print(colors.fail + '☒' + colors.close, end=' ')\n",
    "        print(guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們可以看到在30次的訓練循環之後,我們己經可以在驗證準確性上達到99.8%的程度。\n",
    "\n",
    "以上方法的一個先行條件是它假設:給定固定長度的序列當輸入[... t]有可能生成固定長度的目標[...t]序列。\n",
    "\n",
    "這在某些情況下可行，但不適用於大多數使用情境。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一般情境：序列到序列(seq-to-seq)的典型範例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在一般情況下，輸入序列和輸出序列具有不同的長度（例如機器翻譯），並且為了開始預測目標，需要整個輸入序列。這需要更高級的設置，這是人們在沒有更多的上下文的情況下提到“序列到序列模型”時經常提到的。這是如何工作的：\n",
    "1. RNN層（或多個RNN層的堆疊）作為“編碼器(encoder)”：它處理輸入序列並返回其自身的內部狀態。請注意，我們丟棄編碼器RNN的輸出，只保留它的內部狀態。這個狀態將作為下一步解碼器的“上下文”或“條件”。\n",
    "2. 另一個RNN層（或多個RNN層的堆疊）充當“解碼器(decoder)”：對給定的目標序列的先前字符進行訓練，以預測目標序列的下一個字符。具體而言，訓練是將目標序列轉換成相同的序列偏移(offset)一個步驟的過程，這種情況稱為“教師強制(teacher forcing)”的訓練過程。重要的是，解碼器(decoder)使用來自編碼器(encoder)的狀態向量作為初始狀態，這是解碼器如何獲得關於它應該產生何種產出的關鍵資訊。實際上，解碼器學習以輸入序列為條件生成給定目標[... t]的目標[t + 1 ...]。\n",
    "\n",
    "![teacher_forcing](https://blog.keras.io/img/seq2seq/seq2seq-teacher-forcing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在預測模式下，當我們想要解碼(decode)未知的輸入序列時，我們經歷一個稍微不同的過程：\n",
    "1. 將輸入序列編碼成狀態向量(hidden state vector)。\n",
    "2. 從大小為1的目標序列開始（只是開始序列字符）。\n",
    "3. 將狀態向量和1-char目標序列饋送給解碼器以產生下一個字符的預測。\n",
    "4. 使用這些預測對下一個字符進行採樣（我們簡單地使用argmax）。\n",
    "5. 將採樣的字符附加到目標序列。\n",
    "6. 重複，直到我們拿到生成序列結束字符或我們達到字符限制。\n",
    "\n",
    "![seq-to-seq-decoder](https://blog.keras.io/img/seq2seq/seq2seq-inference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以使用相同的過程來訓練Seq2Seq網絡，而不需要“教師強制”，即通過將解碼器的預測重新輸入到解碼器中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "讓我們用實際的程式碼來說明這些想法。\n",
    "\n",
    "為了實現我們的範例，我們將使用英語句子對應的中文語句翻譯的數據集，您可以從[[manythings.org/anki](http://manythings.org/anki)]下載這些數據集。\n",
    "要下載的文件被稱為cmn-eng.zip(簡中對應到英文)。為了更貼近學習的效果, 我己經把簡中轉成了繁中的版本（cmn-tw.txt），可以從[Github](https://github.com/erhwenkuo/deep-learning-with-keras-notebooks/blob/master/assets/data/cmn-tw.txt)上取得這個資料檔。我們將實現一個字符級(character-level)的序列到序列模型，逐個字符地處理輸入，並逐個字符地產生輸出。另一個選擇是一個字級(word-level)模型，這個模型往往是機器翻譯更常見的。在這篇文章的最後，你會發現一些關於使用嵌入圖層(embedding layers)將我們的模型轉換為字級模型的參考連結。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料準備\n",
    "* 從[Github](https://github.com/erhwenkuo/deep-learning-with-keras-notebooks/blob/master/assets/data/cmn-tw.txt)下載cmn-tw.txt檔案。\n",
    "* 在這個Jupyter Notebook所在的目錄下產生一個新的子目錄\"data\"。\n",
    "* 把下載的資料檔複製到\"data\"的目錄裡頭。\n",
    "\n",
    "最後你的目錄結構看起來像這樣:\n",
    "```\n",
    "xxx.ipynb\n",
    "data/   \n",
    "└── cmn-tw.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是我們的流程總結：\n",
    "1. 將句子(sentence)轉換為3個Numpy數組, `encoder_input_data`, `decoder_input_data`, `decoder_target_data`：\n",
    "  * `encoder_input_data`是包含英文句子的one-hot向量化的三維形狀數組（num_pairs, max_english_sentence_length, num_english_characters）。\n",
    "  * `decoder_input_data`是包含中文句子的one-hot向量化的三維形狀數組（num_pairs, max_chinese_sentence_length, num_chinese_characters）。\n",
    "  * `decoder_target_data`與`decoder_input_data`相同，但是偏移了一個時間步長。 `decoder_target_data` [:,t,：]將與`decoder_input_data` [：,t+1,：]相同。\n",
    "  \n",
    "2. 訓練一個基本的基於LSTM的Seq2Seq模型來預測給出`encoder_input_data`和`decoder_input_data`的`decoder_target_data`。我們的模型使用教師強制(teacher forcing)的手法。\n",
    "3. 解碼一些句子以檢查模型是否正常工作（將來自`encoder_input_data`的樣本轉換為來自`decoder_target_data`的對應樣本）。\n",
    "\n",
    "\n",
    "整個網絡的架構構建可以參考以下的圖示:\n",
    "\n",
    "![seq2seq](https://upload-images.jianshu.io/upload_images/1667471-dc52883e89b07014.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/646)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1. 引入相關的函數庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 專案的根目錄路徑\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# 置放訓練資料的目錄\n",
    "DATA_PATH = os.path.join(ROOT_DIR, \"data\")\n",
    "\n",
    "# 訓練資料檔\n",
    "DATA_FILE = os.path.join(DATA_PATH, \"cmn-tw.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2. 相關的參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # 訓練時的批次數量\n",
    "epochs = 100 # 訓練循環數\n",
    "latent_dim = 256 # 編碼後的潛在空間的維度(dimensions of latent space)\n",
    "num_samples = 10000 # 用來訓練的樣本數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.資料的前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料向量化\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set() # 英文字符集\n",
    "target_characters = set() # 中文字符集\n",
    "lines = open(DATA_FILE, mode=\"r\", encoding=\"utf-8\").read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逐行的讀取與處理\n",
    "for line in lines[: min(num_samples, len(lines)-1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    \n",
    "    # 我們使用“tab”作為“開始序列[SOS]”字符或目標，“\\n”作為“結束序列[EOS]”字符。 <-- **重要\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    \n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 20000\n",
      "Number of unique input tokens: 73\n",
      "Number of unique output tokens: 2165\n",
      "Max sequence length for inputs: 33\n",
      "Max sequence length for outputs: 22\n"
     ]
    }
   ],
   "source": [
    "input_characters = sorted(list(input_characters)) # 全部輸入的字符集\n",
    "target_characters = sorted(list(target_characters)) # 全部目標字符集\n",
    "\n",
    "num_encoder_tokens = len(input_characters) # 所有輸入字符的數量\n",
    "num_decoder_tokens = len(target_characters) # 所有輸目標字符的數量\n",
    "\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts]) # 最長的輸入句子長度\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts]) # 最長的目標句子長度\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 輸入字符的索引字典\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "\n",
    "# 輸目標字符的索引字典\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "# 包含英文句子的one-hot向量化的三維形狀數組（num_pairs，max_english_sentence_length，num_english_characters）\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# 包含中文句子的one-hot向量化的三維形狀數組（num_pairs，max_chinese_sentence_length，num_chinese_characters）\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# decoder_target_data與decoder_input_data相同，但是偏移了一個時間步長。 \n",
    "# decoder_target_data [:, t，：]將與decoder_input_data [：，t + 1，：]相同\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# 把資料轉換成要用來訓練用的張量資料結構 <-- 重要\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "        \n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 22, 2165)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 33, 73)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4.構建網絡架構\n",
    "\n",
    "![seq2seq_translation](https://camo.githubusercontent.com/44a4c60ee9446a14effc6057a16c9f12b61102b5/68747470733a2f2f692e696d6775722e636f6d2f4e7a766c4733582e706e67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_encoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, None, 73)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, None, 2165)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (LSTM)             [(None, 256), (None, 337920      encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 256),  2480128     decoder_input[0][0]              \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_output (Dense)          (None, None, 2165)   556405      decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 3,374,453\n",
      "Trainable params: 3,374,453\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ===== 編碼 (encoder) ====\n",
    "\n",
    "# 定義輸入的序列\n",
    "# 注意：因為輸入序列長度(timesteps)可變的情況，使用input_shape =（None，num_features）\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens), name='encoder_input') \n",
    "encoder = LSTM(latent_dim, return_state=True, name='encoder_lstm') # 需要取得LSTM的內部state, 因此設定\"return_state=True\"\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# 我們拋棄掉`encoder_outputs`因為我們只需要LSTM cell的內部state參數\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# ==== 解碼 (decoder) ====\n",
    "\n",
    "# 設定解碼器(decoder)\n",
    "# 注意：因為輸出序列的長度(timesteps)是變動的，使用input_shape =（None，num_features）\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), name='decoder_input')\n",
    "\n",
    "# 我們設定我們的解碼器回傳整個輸出的序列同時也回傳內部的states參數\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "\n",
    "# 在訓練時我們不會使用這些回傳的states, 但是在預測時我們會用到這些states參數\n",
    "# **解碼器的初始狀態是使用編碼器的最後的狀態(states)**\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states) #我們使用`encoder_states`來做為初始值(initial state) <-- 重要\n",
    "\n",
    "# 接密集層(dense)來進行softmax運算每一個字符可能的機率\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_output')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# 定義一個模型接收encoder_input_data` & `decoder_input_data`做為輸入而輸出`decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# 打印出模型結構\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-91dc8be1bd14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 產生網絡拓撲圖\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'seq2seq_graph.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'seq2seq_graph.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\program\\Anaconda\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \"\"\"\n\u001b[1;32m--> 132\u001b[1;33m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\program\\Anaconda\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rankdir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\program\\Anaconda\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         raise ImportError(\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[1;34m'Failed to import `pydot`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[1;34m'Please install `pydot`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[1;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "# 產生網絡拓撲圖\n",
    "plot_model(model, to_file='seq2seq_graph.png')\n",
    "Image('seq2seq_graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5.訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 245s 15ms/step - loss: 0.7701 - val_loss: 1.0907\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 242s 15ms/step - loss: 0.7355 - val_loss: 1.0473\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 243s 15ms/step - loss: 0.7072 - val_loss: 1.0157\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 239s 15ms/step - loss: 0.6808 - val_loss: 0.9848\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 239s 15ms/step - loss: 0.6548 - val_loss: 0.9557\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 241s 15ms/step - loss: 0.6343 - val_loss: 0.9253\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 244s 15ms/step - loss: 0.6096 - val_loss: 0.8983\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 244s 15ms/step - loss: 0.5872 - val_loss: 0.8716\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 246s 15ms/step - loss: 0.5667 - val_loss: 0.8453\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 243s 15ms/step - loss: 0.5473 - val_loss: 0.8214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\program\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer decoder_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'encoder_lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# 設定模型超參數\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# 開始訓練\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_split=0.2)\n",
    "\n",
    "# 儲存模型\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 6.模型預測\n",
    "\n",
    "以下是預測階段的步驟:\n",
    "\n",
    "1. 對輸入進行編碼(encode)並取得解碼器所需要的初始狀態(initial decoder state)\n",
    "2. 以此初始狀態運行一步解碼器，並以“開始序列”標記作為目標。輸出將是下一個目標標記\n",
    "3. 重複當前目標標記和當前狀態\n",
    "\n",
    "\n",
    "![seq2seq_predict](https://4.bp.blogspot.com/-6DALk3-hPtA/WO04i5GgXLI/AAAAAAAABtc/2t9mYz4nQDg9jLoHdTkywDUfxIOFJfC_gCLcB/s640/Seq2SeqDiagram.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: 把它放在那裡。\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: 把它放在那裡。\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: 別再念了。\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: 去這兒等一樣。\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: 沒人。\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: 我想我的。\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: 我想要做。\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: 不要！\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: 這是你的書。\n",
      "\n",
      "-\n",
      "Input sentence: He ran.\n",
      "Decoded sentence: 他看起來很有錢。\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: 做到事。\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: 我喜歡跑步。\n",
      "\n",
      "-\n",
      "Input sentence: I quit.\n",
      "Decoded sentence: 我想我的。\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: 我是個好人。\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: 把它放在那兒。\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: 沒有！\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: 沒有！\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: 別再念了。\n",
      "\n",
      "-\n",
      "Input sentence: Try it.\n",
      "Decoded sentence: 把它給我們。\n",
      "\n",
      "-\n",
      "Input sentence: We try.\n",
      "Decoded sentence: 我們會來。\n",
      "\n",
      "-\n",
      "Input sentence: Why me?\n",
      "Decoded sentence: 我們為甚麼失敗了？\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: 回答我。\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: 會議下來。\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: 有人在看電視。\n",
      "\n",
      "-\n",
      "Input sentence: Be kind.\n",
      "Decoded sentence: 有人在看電視。\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: 有人在看電視。\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: 快點。\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: 快點。\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: 你好。\n",
      "\n",
      "-\n",
      "Input sentence: Get Tom.\n",
      "Decoded sentence: 走開！\n",
      "\n",
      "-\n",
      "Input sentence: Get out!\n",
      "Decoded sentence: 走開！\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: 走開！\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: 走開！\n",
      "\n",
      "-\n",
      "Input sentence: Go away.\n",
      "Decoded sentence: 走開！\n",
      "\n",
      "-\n",
      "Input sentence: Goodbye!\n",
      "Decoded sentence: 走開！\n",
      "\n",
      "-\n",
      "Input sentence: Goodbye!\n",
      "Decoded sentence: 走開！\n",
      "\n",
      "-\n",
      "Input sentence: Hang on!\n",
      "Decoded sentence: 你好。\n",
      "\n",
      "-\n",
      "Input sentence: He came.\n",
      "Decoded sentence: 他開始哭了。\n",
      "\n",
      "-\n",
      "Input sentence: He runs.\n",
      "Decoded sentence: 他看起來很有錢。\n",
      "\n",
      "-\n",
      "Input sentence: Help me.\n",
      "Decoded sentence: 好。\n",
      "\n",
      "-\n",
      "Input sentence: Hold on.\n",
      "Decoded sentence: 你一起來。\n",
      "\n",
      "-\n",
      "Input sentence: Hug Tom.\n",
      "Decoded sentence: 你可以來。\n",
      "\n",
      "-\n",
      "Input sentence: I agree.\n",
      "Decoded sentence: 我看到了他在車。\n",
      "\n",
      "-\n",
      "Input sentence: I'm ill.\n",
      "Decoded sentence: 我是個好人。\n",
      "\n",
      "-\n",
      "Input sentence: I'm old.\n",
      "Decoded sentence: 我是個好人。\n",
      "\n",
      "-\n",
      "Input sentence: It's OK.\n",
      "Decoded sentence: 它是我的錯。\n",
      "\n",
      "-\n",
      "Input sentence: It's me.\n",
      "Decoded sentence: 它是我的錯。\n",
      "\n",
      "-\n",
      "Input sentence: Join us.\n",
      "Decoded sentence: 把它放在那兒。\n",
      "\n",
      "-\n",
      "Input sentence: Keep it.\n",
      "Decoded sentence: 把它放在那兒。\n",
      "\n",
      "-\n",
      "Input sentence: Kiss me.\n",
      "Decoded sentence: 把它放在那兒。\n",
      "\n",
      "-\n",
      "Input sentence: Perfect!\n",
      "Decoded sentence: 有人來了。\n",
      "\n",
      "-\n",
      "Input sentence: See you.\n",
      "Decoded sentence: 再見！\n",
      "\n",
      "-\n",
      "Input sentence: Shut up!\n",
      "Decoded sentence: 你真丟臉！\n",
      "\n",
      "-\n",
      "Input sentence: Skip it.\n",
      "Decoded sentence: 別再傻著。\n",
      "\n",
      "-\n",
      "Input sentence: Take it.\n",
      "Decoded sentence: 別再傻著。\n",
      "\n",
      "-\n",
      "Input sentence: Wake up!\n",
      "Decoded sentence: 去了一下！\n",
      "\n",
      "-\n",
      "Input sentence: Wash up.\n",
      "Decoded sentence: 去這兒等。\n",
      "\n",
      "-\n",
      "Input sentence: We know.\n",
      "Decoded sentence: 我們會做。\n",
      "\n",
      "-\n",
      "Input sentence: Welcome.\n",
      "Decoded sentence: 我們會來。\n",
      "\n",
      "-\n",
      "Input sentence: Who won?\n",
      "Decoded sentence: 誰說了？\n",
      "\n",
      "-\n",
      "Input sentence: Why not?\n",
      "Decoded sentence: 我們為甚麼失敗了？\n",
      "\n",
      "-\n",
      "Input sentence: You run.\n",
      "Decoded sentence: 你會說話。\n",
      "\n",
      "-\n",
      "Input sentence: Back off.\n",
      "Decoded sentence: 有人在看電視。\n",
      "\n",
      "-\n",
      "Input sentence: Be still.\n",
      "Decoded sentence: 有人在看電視。\n",
      "\n",
      "-\n",
      "Input sentence: Cuff him.\n",
      "Decoded sentence: 把我的行車!\n",
      "\n",
      "-\n",
      "Input sentence: Drive on.\n",
      "Decoded sentence: 開車慢點。\n",
      "\n",
      "-\n",
      "Input sentence: Get away!\n",
      "Decoded sentence: 走開！\n",
      "\n",
      "-\n",
      "Input sentence: Get away!\n",
      "Decoded sentence: 走開！\n",
      "\n",
      "-\n",
      "Input sentence: Get down!\n",
      "Decoded sentence: 走開！\n",
      "\n",
      "-\n",
      "Input sentence: Get lost!\n",
      "Decoded sentence: 走開！\n",
      "\n",
      "-\n",
      "Input sentence: Get real.\n",
      "Decoded sentence: 走開！\n",
      "\n",
      "-\n",
      "Input sentence: Grab Tom.\n",
      "Decoded sentence: 抓住湯姆。\n",
      "\n",
      "-\n",
      "Input sentence: Grab him.\n",
      "Decoded sentence: 抓住湯姆。\n",
      "\n",
      "-\n",
      "Input sentence: Have fun.\n",
      "Decoded sentence: 進來。\n",
      "\n",
      "-\n",
      "Input sentence: He tries.\n",
      "Decoded sentence: 他喜歡茶。\n",
      "\n",
      "-\n",
      "Input sentence: Humor me.\n",
      "Decoded sentence: 你可以來了。\n",
      "\n",
      "-\n",
      "Input sentence: Hurry up.\n",
      "Decoded sentence: 快點！\n",
      "\n",
      "-\n",
      "Input sentence: Hurry up.\n",
      "Decoded sentence: 快點！\n",
      "\n",
      "-\n",
      "Input sentence: I forgot.\n",
      "Decoded sentence: 我覺得很孤獨。\n",
      "\n",
      "-\n",
      "Input sentence: I resign.\n",
      "Decoded sentence: 我吃了。\n",
      "\n",
      "-\n",
      "Input sentence: I'll pay.\n",
      "Decoded sentence: 我會在這裡吃飯。\n",
      "\n",
      "-\n",
      "Input sentence: I'm busy.\n",
      "Decoded sentence: 我是個好人。\n",
      "\n",
      "-\n",
      "Input sentence: I'm cold.\n",
      "Decoded sentence: 我是個好人。\n",
      "\n",
      "-\n",
      "Input sentence: I'm fine.\n",
      "Decoded sentence: 我是個好人。\n",
      "\n",
      "-\n",
      "Input sentence: I'm full.\n",
      "Decoded sentence: 我是個好人。\n",
      "\n",
      "-\n",
      "Input sentence: I'm sick.\n",
      "Decoded sentence: 我是個好人。\n",
      "\n",
      "-\n",
      "Input sentence: I'm sick.\n",
      "Decoded sentence: 我是個好人。\n",
      "\n",
      "-\n",
      "Input sentence: Leave me.\n",
      "Decoded sentence: 讓我一樣。\n",
      "\n",
      "-\n",
      "Input sentence: Let's go!\n",
      "Decoded sentence: 讓我們一想吧。\n",
      "\n",
      "-\n",
      "Input sentence: Let's go!\n",
      "Decoded sentence: 讓我們一想吧。\n",
      "\n",
      "-\n",
      "Input sentence: Let's go!\n",
      "Decoded sentence: 讓我們一想吧。\n",
      "\n",
      "-\n",
      "Input sentence: Look out!\n",
      "Decoded sentence: 看看起來。\n",
      "\n",
      "-\n",
      "Input sentence: She runs.\n",
      "Decoded sentence: 她是我的同。\n",
      "\n",
      "-\n",
      "Input sentence: Stand up.\n",
      "Decoded sentence: 別再念了。\n",
      "\n",
      "-\n",
      "Input sentence: They won.\n",
      "Decoded sentence: 他們都喜歡吃。\n",
      "\n",
      "-\n",
      "Input sentence: Tom died.\n",
      "Decoded sentence: 湯姆在家。\n",
      "\n",
      "-\n",
      "Input sentence: Tom quit.\n",
      "Decoded sentence: 湯姆在家。\n",
      "\n",
      "-\n",
      "Input sentence: Tom swam.\n",
      "Decoded sentence: 湯姆在家。\n",
      "\n",
      "-\n",
      "Input sentence: Trust me.\n",
      "Decoded sentence: 把它放在那兒。\n",
      "\n",
      "-\n",
      "Input sentence: Try hard.\n",
      "Decoded sentence: 試試吧。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 定義要進行取樣的模型\n",
    "\n",
    "# 定義編碼器(encoder)的模型\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# 定義解碼器LSTM cell的初始權重輸入\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# # 解碼器(decoder)定義初始狀態(initial decoder state)\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs) #我們使用`decoder_states_inputs`來做為初始值(initial state)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# 定義解碼器(decoder)的模型\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "\n",
    "# 反向查找字符索引來將序列解碼為可讀的內容。\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "# 對序列進行解碼\n",
    "def decode_sequence(input_seq):\n",
    "    # 將輸入編碼成為state向量\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # 產生長度為1的空白目標序列\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    \n",
    "    # 發佈特定的目標序列起始字符\"[SOS]\",在這個範例中是使用 \"\\t\"字符\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # 對批次的序列進行抽樣迴圈\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # 對符標抽樣\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # 停止迴圈的條件: 到達最大的長度或是找到\"停止[EOS]\"字符,在這個範例中是使用 \"\\n\"字符\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 更新目標序列(of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 更新 states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # 從訓練集中取出一個序列並試著解碼\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "參考:\n",
    "* [A ten-minute introduction to sequence-to-sequence learning in Keras](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "MIT License\n",
    "\n",
    "Copyright (c) 2018 Erhwen Kuo\n",
    "\n",
    "Copyright (c) 2017 François Chollet\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
